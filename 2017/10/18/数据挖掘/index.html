<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.0.0-rc1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Data Mining 学习笔记背景我们处在信息时代，这个时代不缺乏数据,数据库中的数据量急速膨胀，但是缺乏有价值的信息(当然也缺乏获取有用信息的人)。 于是产生了KDD(knowledge discovery in dadabase),Data Mining 是KDD的一个步骤。">
<meta property="og:type" content="article">
<meta property="og:title" content="数据挖掘">
<meta property="og:url" content="http://yoursite.com/2017/10/18/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/index.html">
<meta property="og:site_name" content="DoubleMagicXu的博客">
<meta property="og:description" content="Data Mining 学习笔记背景我们处在信息时代，这个时代不缺乏数据,数据库中的数据量急速膨胀，但是缺乏有价值的信息(当然也缺乏获取有用信息的人)。 于是产生了KDD(knowledge discovery in dadabase),Data Mining 是KDD的一个步骤。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2022/04/03/XSqWtCRUPwZkspe.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/03/xNIJ81YdTVGirpy.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/03/vIA365BLXwbZ29G.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/03/97OMI8NbXQZsqLa.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/03/R28mXKFGPcbD3L1.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/03/h7f5jwMbkQV9m8e.png">
<meta property="og:image" content="https://s2.loli.net/2023/03/29/sgTyGYLbV4Wnl6c.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/03/GxkpMqOcJQD9sn1.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/03/NuiQPa8A5VFoLfk.png">
<meta property="og:image" content="https://s2.loli.net/2022/04/03/DjMnrBEhYq6CI39.png">
<meta property="article:published_time" content="2017-10-18T15:35:50.000Z">
<meta property="article:modified_time" content="2023-03-29T09:20:39.575Z">
<meta property="article:author" content="DoubleMagicXu">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2022/04/03/XSqWtCRUPwZkspe.png">

<link rel="canonical" href="http://yoursite.com/2017/10/18/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>数据挖掘 | DoubleMagicXu的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">DoubleMagicXu的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/10/18/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="DoubleMagicXu">
      <meta itemprop="description" content="晚上吃什么">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DoubleMagicXu的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          数据挖掘
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-10-18 23:35:50" itemprop="dateCreated datePublished" datetime="2017-10-18T23:35:50+08:00">2017-10-18</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Data-Mining-学习笔记"><a href="#Data-Mining-学习笔记" class="headerlink" title="Data Mining 学习笔记"></a>Data Mining 学习笔记</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>我们处在信息时代，这个时代不缺乏数据,数据库中的数据量急速膨胀，但是缺乏有价值的信息(当然也缺乏获取有用信息的人)。</p>
<p>于是产生了KDD(knowledge discovery in dadabase),Data Mining 是KDD的一个步骤。<br><span id="more"></span></p>
<h2 id="Data-Mining-概念"><a href="#Data-Mining-概念" class="headerlink" title="Data Mining 概念"></a>Data Mining 概念</h2><p>从<em>大量的,不完全的，有噪声的，模糊的，随机的</em>数据中,提取<em>隐含在其中的，人们事先不知道的，但又是潜在信息和知识</em>的过程。</p>
<p>知识发现(KDD)是“数据挖掘”的广义说法；数据挖掘是知识发现过程的核心。</p>
<h2 id="Similarity-and-Dissimilarity"><a href="#Similarity-and-Dissimilarity" class="headerlink" title="Similarity and Dissimilarity"></a>Similarity and Dissimilarity</h2><p>相似度一般取值[0,1],而不相似度最小取0(eg:Distace)</p>
<h3 id="Minkowski-Distance-明式距离"><a href="#Minkowski-Distance-明式距离" class="headerlink" title="Minkowski Distance(明式距离)"></a>Minkowski Distance(明式距离)</h3><p>$$\sqrt[h]{\sum|x_i-y_i|^h}$$<br>又被成为L-h norm</p>
<p><strong>特殊情况</strong></p>
<pre><code>1. 哈弗曼距离(L-1 norm)
2. 欧氏距离(L-2 norm)
3. supuremum距离，或者称为棋盘距离
</code></pre><h3 id="Cosin-Similarity-余弦相似度"><a href="#Cosin-Similarity-余弦相似度" class="headerlink" title="Cosin Similarity(余弦相似度)"></a>Cosin Similarity(余弦相似度)</h3><p>$$\cos(\theta)=\frac{a\cdot b}{||a||\times||b||}$$</p>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><h3 id="Data-Preprocessing-主要步骤"><a href="#Data-Preprocessing-主要步骤" class="headerlink" title="Data Preprocessing 主要步骤"></a>Data Preprocessing 主要步骤</h3><pre><code>1. Data Cleaning（missing,noisy,inconsistent)
2. Data Integration
3. Data Reduction
4. Data Transformation
</code></pre><h3 id="Data-Cleaning-处理missing-data方法："><a href="#Data-Cleaning-处理missing-data方法：" class="headerlink" title="Data Cleaning:处理missing data方法："></a>Data Cleaning:处理<em>missing data</em>方法：</h3><p> the most probable value: inference-based （基于推理的）such as Bayesian formula or decision tree.</p>
<h3 id="Data-Cleaning-处理noisy-data方法"><a href="#Data-Cleaning-处理noisy-data方法" class="headerlink" title="Data Cleaning:处理noisy data方法:"></a>Data Cleaning:处理<em>noisy data</em>方法:</h3><p><strong>Binning （分级）</strong></p>
<p><em>first sort data and partition into (equal-frequency) bins</em><br><em>then one can smooth by bin means, smooth by bin median, smooth by bin boundaries, etc.</em></p>
<p><strong>Regression</strong></p>
<p><em>smooth by fitting the data into regression functions</em></p>
<p><strong>Clustering</strong></p>
<p><em>detect and remove outliers</em></p>
<p><strong>Combined computer and human inspection （人机检查）</strong></p>
<p><em>detect suspicious （可疑的） values and check by human (e.g., deal with possible outliers)</em></p>
<h3 id="Data-Integration-数据整合"><a href="#Data-Integration-数据整合" class="headerlink" title="Data Integration(数据整合)"></a>Data Integration(数据整合)</h3><p>含义：Combines data from multiple sources into a coherent store （统一存储）</p>
<p><strong>Handling Redundancy in Data Integration</strong></p>
<pre><code>1. 不同属性表示同一个意思(Object identification)
2. 派生数据(Derivable data)
</code></pre><p><strong>Detection of redundant attributes</strong></p>
<pre><code>1.  correlation analysis
2.  covariance analysis
</code></pre><p><img src="https://s2.loli.net/2022/04/03/XSqWtCRUPwZkspe.png" alt=""><br>${\sigma} _A \sigma_B$是标准差</p>
<p> $r_{A,B}&gt;0$ ,正相关</p>
<p> $r_{A,B}=0$ : 独立</p>
<p>$r_{A,B}&lt;0$ : 负相关</p>
<p>$$<br>ov(A,B)=E(A\cdot B)-\bar A\bar B<br>$$<br><img src="https://s2.loli.net/2022/04/03/xNIJ81YdTVGirpy.png" alt=""></p>
<h3 id="Data-Reduction"><a href="#Data-Reduction" class="headerlink" title="Data Reduction"></a>Data Reduction</h3><p><strong>方法：</strong></p>
<pre><code>1. Dimensionality reduction
2. Numerosity reduction
3. Data compression
</code></pre><p><strong>Dimensionality reduction</strong></p>
<p>含义：remove unimportant attributes</p>
<p>方法：</p>
<pre><code>1. Wavelet transforms(小波变换)
2. Principal Components Analysis (PCA)
3. Feature subset selection, feature creation
</code></pre><p>特征提取与特征选择</p>
<pre><code>特征提取通过投影变换降维，它生成新特征。典型用途：图像，文档特征提取。
特征选择从给定高维数据中选出一组最具描述性的有效特征，不生成新特征。典型用途：基因选择。
</code></pre><p><strong>Numerosity Reduction</strong></p>
<p>含义：Reduce data volume by choosing alternative, smaller forms (in volume ) of data representation</p>
<p>方法：</p>
<pre><code>1. Parametric methods
2. Non-parametric methods
</code></pre><p>Parametric Data Reduction</p>
<pre><code>1. Linear regression
2. Multiple regression
3. Log-linear model
</code></pre><p> Non-parametric Data Reduction</p>
<pre><code>1. histograms
2. clustering
3. sampling
</code></pre><p><strong>Data Compression</strong></p>
<p>含义：</p>
<p>A function that maps the entire set of values of a given attribute to a new set of replacement values s.t. each old value can be identified with one of the new values.</p>
<p>方法：</p>
<pre><code>1. Smoothing: Remove noise from data
2. Attribute/feature construction
3. Aggregation(聚合)
4. Normalization: Scaled to fall within a smaller, specified range
</code></pre><h2 id="关联规则"><a href="#关联规则" class="headerlink" title="关联规则"></a>关联规则</h2><p>概念：项集，事物，关联规则，事物标识</p>
<p><strong>项集</strong></p>
<p>任意项的集合</p>
<p><strong>k-项集</strong></p>
<p>包含k个项的项集</p>
<p><strong>频繁项集</strong></p>
<p>概念：大于等于最小支持度的项集</p>
<p><strong>支持度</strong></p>
<p>S(A=&gt;B): D中包含 A 和 B 的事务数与总的事务数的比值</p>
<p><strong>可信度</strong></p>
<p>confidence(A =&gt; B )=P(B|A)</p>
<p><strong>强规则</strong></p>
<p>通常定义为那些满足最小支持度和最小可信度的规则.</p>
<pre><code>1. 找出所有的频繁项集(满足最小支持度)
2. 找出所有的强关联规则()由频繁项集生成关联规则,保留满足最小可信度的规则).
</code></pre><h3 id="Apriori-算法-先验算法"><a href="#Apriori-算法-先验算法" class="headerlink" title="Apriori 算法(先验算法)"></a>Apriori 算法(先验算法)</h3><p><strong>中心思想</strong></p>
<p>由频繁(k-1)-项集构建候选k-项集</p>
<p><strong>方法</strong></p>
<pre><code>1. 找到所有的频繁1-项集
2. 扩展频繁(k-1)-项集得到候选k-项集
3. 剪除不满足最小支持度的候选项集
</code></pre><p><strong>Apriori 剪枝原理</strong></p>
<p> 若任一项集是不频繁的,则其超集不应该被生成/测试!</p>
<p> <img src="https://s2.loli.net/2022/04/03/vIA365BLXwbZ29G.png" alt=""></p>
<h3 id="FP-Growth算法"><a href="#FP-Growth算法" class="headerlink" title="FP Growth算法"></a>FP Growth算法</h3><pre><code>1. 扫描事务数据库D一次,得到频繁项的集合F及它们的支持度.将F按支持度降序排列成L,L是频繁项的列表.
2. 创建FP-树的根, 标注其为NULL.对D中的每个事务进行以下操作:根据 L中的次序对事务中的频繁项进行选择和排序. 设事务中的已排序的频繁项列表为[p|P],其中p表示第一个元素,P表示剩余的列表.调用insert_Tree([p|P],T).
</code></pre><p><img src="https://s2.loli.net/2022/04/03/97OMI8NbXQZsqLa.png" alt=""><br><img src="https://s2.loli.net/2022/04/03/R28mXKFGPcbD3L1.png" alt=""></p>
<h2 id="Data-Classification"><a href="#Data-Classification" class="headerlink" title="Data Classification"></a>Data Classification</h2><p><strong>概念：</strong></p>
<p>分类是指把数据样本映射到一个事先定义的类中的学习过程.有监督学习。</p>
<h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><p><strong>概念:</strong></p>
<pre><code>1. 适用于离散值属性、连续值属性
2. 采用自顶向下的递归方式产生一个类似于流程图的树结构
3. 在根节点和各内部节点上选择合适的描述属性，并且根据该属性的不同取值向下建立分枝  
</code></pre><h4 id="决策树算法ID3"><a href="#决策树算法ID3" class="headerlink" title="决策树算法ID3"></a>决策树算法ID3</h4><p><img src="https://s2.loli.net/2022/04/03/h7f5jwMbkQV9m8e.png" alt=""></p>
<p>缺点：</p>
<pre><code>1. ID3是采用“信息增益”来选择分裂属性的。虽然这是一种有效的方法，但其具有明显的倾向性，即它倾向于选择取值较多的属性;
2. ID3算法只能对描述属性为离散型属性的数据集构造决策树
</code></pre><h4 id="决策树算法C4-5"><a href="#决策树算法C4-5" class="headerlink" title="决策树算法Ｃ4.5"></a>决策树算法Ｃ4.5</h4><p><strong>概念：</strong></p>
<p>C4.5既可以处理离散型描述属性，也可以处理连续型描述属性</p>
<p>步骤：</p>
<pre><code>1. 对于连续值描述属性，C4.5将其转换为离散值属性
2. 把某个结点上的数据按照连续型描述属性的具体取值，由小到大进行排序
3. 在&#123;A1c,A2c,…,Atotalc&#125;中生成total-1个分割点
4. 第i个分割点的取值设置vi=(Aic+A(i+1)c)/2
5. 每个分割点将数据集划分为两个子集
6. 挑选最适合的分割点对连续属性离散化
</code></pre><h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><p><strong>概念：</strong></p>
<pre><code>1. 可以分*线性*以及*非线性*数据
2. 通过非线性映射(noliner mapping)把原始训练数据转换到高维
3. 在新维度里寻找超平面(hyperplane)，超平面可以将两类分开
4. 通过support vectors 以及 margins 来寻找超平面
</code></pre><h3 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h3><p><strong>lazy learning vs eager learning</strong></p>
<p>Lazy learning (e.g., instance-based learning): Simply stores training data (or only minor processing) and waits until it is given a test tuple</p>
<p>Eager learning: Given a set of training tuples, constructs a classification model before receiving new (e.g., test) data to classify<br>Lazy: less time in training but more time in predicting</p>
<p><strong>Top 10 Data Mining Algorithm</strong></p>
<pre><code>1. C4.5
2. k-means
3. SVM (Support Vector Machines)
4. Apriori
5. EM (Expectation Maximization)
6. PageRank (网页排名)
7. AdaBoost
8. kNN
9. Naive Bayes
10. CART
</code></pre><h3 id="Bayesian-Networks-and-Classification"><a href="#Bayesian-Networks-and-Classification" class="headerlink" title="Bayesian Networks and Classification"></a>Bayesian Networks and Classification</h3><p><strong>Two components:</strong></p>
<p>(1) A directed acyclic graph 有向无环图  (called a structure)  </p>
<p>(2) a set of conditional probability tables (CPTs)</p>
<p><strong>概念</strong></p>
<p>先验概率：根据历史的资料或主观判断所确定的各种时间发生的概率</p>
<p>后验概率：通过贝叶斯公式，结合调查等方式获取了新的附加信息，对先验概率修正后得到的更符合实际的概率</p>
<p>条件概率：某事件发生后该事件的发生概率</p>
<p>条件概率公式：$P(A|B)=\frac{P(A)P(B|A)}{P(B)}$</p>
<p>全概率公式：$P(A)=\sum_{i=1}^{n}P(B_i)P(A|B_i)$</p>
<p>贝叶斯公式：$P(B_i|A)=\frac{P{B_i}P(A|B<em>i)}{\sum</em>{i=1}^{n}P(B_i)P(A|B_i)}$</p>
<h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><h2 id="Genetic-Algorithms"><a href="#Genetic-Algorithms" class="headerlink" title="Genetic Algorithms"></a>Genetic Algorithms</h2><p><img src="https://s2.loli.net/2023/03/29/sgTyGYLbV4Wnl6c.png" alt=""></p>
<h2 id="粗糙集"><a href="#粗糙集" class="headerlink" title="粗糙集"></a>粗糙集</h2><p><strong>概念：</strong><br>    粗糙集（Rough Set，RS）理论<br>    波兰数学家Z.Pawlak于1982年提出<br>    不完整性和不精确性的数学工具<br>    分析和处理不完备性数据<br>    发现数据间隐藏的关系<br>    揭示潜在规律</p>
<p><strong>等价关系:</strong></p>
<p>设R为定义在集合A上的一个关系，若R是自反的，对称的和传递的，则称R为等价关系。</p>
<p><strong>等价类</strong></p>
<p>设R为集合A上的等价关系，对任何a∈A，集合[a]R={x|x∈A,aRx}称为元素a形成的R等价类。由等价类的定义可知[a]R是非空的，因为a∈[a]R</p>
<p><strong>下近似集</strong></p>
<p>一个知识库K=(U,R)，令XU且R为U上一等价关系，X的下近似集就是对于知识R的能完全确定地归入集合X的对象的集合</p>
<p><strong>上近似集</strong></p>
<p>X的上近似集是知识R的在U中一定和可能归入集合X的对象的集合</p>
<p><strong>正域</strong></p>
<p>$$POSR(X) = R_-(X)$$</p>
<p><strong>负域</strong></p>
<p>$$NEGR(X) = UR^-(X)$$</p>
<p><strong>边界</strong></p>
<p>$$BNR(X) = R^-(X)-R_-(X)$$</p>
<p><strong>由等价关系R描述的对象集X的近似精度为：</strong><br>$$d<em>R(X)=\frac{card(R</em>-(X))}{card(R^{-}(X))}$$</p>
<p>$card(R_-(X))$  $card(R^-(X))$  分别为Ｘ下近似集合、上近似集合中元素的个数。</p>
<p>（1）如果dR(X)=0，则X是R全部不可定义的；</p>
<p>（2）如果dR(X)=1，则X是R全部可定义的；</p>
<p>（3）如果0&lt;dR(X)&lt;1，则X是R部分可定义的。</p>
<p>PR(X)=1-dR(X)反映了定义集合X的粗糙程度，也即不被关系R所描述的程度，称为X的粗糙度。</p>
<p><strong>分类近似的度量</strong></p>
<p>$$d<em>R(F)=\frac{\sum</em>{i=1}^{n}card(R_-(X<em>i))}{\sum</em>{i=1}^{n}card(R^-(X_i))}$$</p>
<p>$$r<em>R(F)=\frac{\sum</em>{i=1}^ncard(R_-(X_i))}{card(U)}$$</p>
<p>两种方式在本质上是等价的</p>
<p><strong>分类近似的度量 – 例子</strong></p>
<p><em>一个知识库K=(U,R),其中U={x1,x2,x3,x4,x5,x6,x7,x8}，一个等价关系R形成的等价类为Y1={x1,x3,x5}, Y2={x2,x4}, Y3={x6,x7,x8}。现由分类F形成的等价类： X1={x1,x2,x4}, X2={x3,x5,x8}, X3={x6,x7}。分析由R描述分类F的近似度。</em></p>
<p>解答：</p>
<pre><code>R_(X1)=Y2 =&#123;x2,x4&#125;
R_(X2)=[]
R_(X3)=[]
R-(X1)= Y1∪Y2= &#123;x1,x2,x3,x4,x5&#125;
R- (X2)= Y1∪Y3= &#123;x1,x3,x5，x6,x7,x8&#125;
R- (X3)= Y3=&#123;x6,x7,x8&#125;
</code></pre><p>​             $$d_R(F)=\frac{2+0+0}{5+6+3}=\frac{1}{7}$$ </p>
<p>​            $$r_R(F)=\frac{2+0+0}{8}=\frac{1}{4}$$</p>
<p> 因此，分类F不能被R完全定义，即部分可定义的。</p>
<p><strong>等价关系简化</strong></p>
<p>对于知识库K = (U,R)，如果存在等价关系r∈R,使得ind(r)=ind(R)，则称r是可省略的，否则，称r是不可省略的。<br>（1）若任意r∈R是不可省略的，则称R是独立的<br>（2）独立等价关系的子集也是独立的</p>
<p>若Q⊂R，ind(Q)=ind(P)，则称Q为P的简化，记做red(P).所有简化的交集为等价关系的核，记做core(P). </p>
<p><strong>知识的相对简化</strong></p>
<p><img src="https://s2.loli.net/2022/04/03/GxkpMqOcJQD9sn1.png" alt=""></p>
<p><img src="https://s2.loli.net/2022/04/03/NuiQPa8A5VFoLfk.png" alt=""></p>
<p><strong>知识依赖性度量</strong></p>
<p>令K = (U, R)是一个知识库，P, Q  R，<br>（1）知识Q依赖于知识P或知识P可以推导知识Q，当且仅当ind(P)ind(Q)，记作P→Q；<br>（2）知识P和知识Q是等价的，当且仅当P→Q且Q →P，即ind(P) = ind(Q)，记作P = Q；<br>（3）知识P和知识Q是独立的，当且仅当 且P⇨Q和Q⇨P均不成立的时候，记作P≠Q。 </p>
<p>$$k=r_P(Q)=\frac{card(POS_P(Q))}{card(U)}$$</p>
<p>令K = (U, R)是一个知识库，P,Q  R，当上式成立时，我们称知识Q是k(0≤k≤1)依赖于知识P，记作P→Q。<br>（1）当k=1时，我们称知识Q是完全依赖于知识P；<br>（2）当0&lt;k&lt;1时，则称知识Q是部分(粗糙)依赖于知识P；<br>（3）当k=0时，则称知识Q完全独立于知识P。 </p>
<p><strong>可辨识矩阵</strong></p>
<p><img src="https://s2.loli.net/2022/04/03/DjMnrBEhYq6CI39.png" alt="image.png"></p>
<h2 id="Clustering-Algorithm-聚类算法"><a href="#Clustering-Algorithm-聚类算法" class="headerlink" title="Clustering Algorithm (聚类算法)"></a>Clustering Algorithm (聚类算法)</h2><h3 id="k-means"><a href="#k-means" class="headerlink" title="k-means"></a>k-means</h3><p><strong>Given k, the k-means algorithm is implemented in four steps:</strong></p>
<pre><code>1. Partition objects into k non-empty subsets
2. Compute seed points as the centroids (质心) of the clusters of the current partition (the centroid is the center, i.e., mean point, of the cluster)
3. Assign each object to the cluster with the nearest seed point  
4. Go back to Step 2, stop when no more new assignment
</code></pre><h3 id="Hierarchical-Clustering-层次聚类"><a href="#Hierarchical-Clustering-层次聚类" class="headerlink" title="Hierarchical Clustering(层次聚类)"></a>Hierarchical Clustering(层次聚类)</h3><p><strong>概念</strong></p>
<p>A hierarchical clustering method works by grouping objects into a tree of clusters.</p>
<p><strong>分类</strong></p>
<p> agglomerative (凝聚)<br> divisive (分裂)</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item"></div>
      <div class="post-nav-item">
    <a href="/2017/10/18/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/" rel="next" title="计算机图形学">
      计算机图形学 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Data-Mining-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0"><span class="nav-number">1.</span> <span class="nav-text">Data Mining 学习笔记</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%83%8C%E6%99%AF"><span class="nav-number">1.1.</span> <span class="nav-text">背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Data-Mining-%E6%A6%82%E5%BF%B5"><span class="nav-number">1.2.</span> <span class="nav-text">Data Mining 概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Similarity-and-Dissimilarity"><span class="nav-number">1.3.</span> <span class="nav-text">Similarity and Dissimilarity</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Minkowski-Distance-%E6%98%8E%E5%BC%8F%E8%B7%9D%E7%A6%BB"><span class="nav-number">1.3.1.</span> <span class="nav-text">Minkowski Distance(明式距离)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cosin-Similarity-%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E5%BA%A6"><span class="nav-number">1.3.2.</span> <span class="nav-text">Cosin Similarity(余弦相似度)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">1.4.</span> <span class="nav-text">数据预处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Data-Preprocessing-%E4%B8%BB%E8%A6%81%E6%AD%A5%E9%AA%A4"><span class="nav-number">1.4.1.</span> <span class="nav-text">Data Preprocessing 主要步骤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Data-Cleaning-%E5%A4%84%E7%90%86missing-data%E6%96%B9%E6%B3%95%EF%BC%9A"><span class="nav-number">1.4.2.</span> <span class="nav-text">Data Cleaning:处理missing data方法：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Data-Cleaning-%E5%A4%84%E7%90%86noisy-data%E6%96%B9%E6%B3%95"><span class="nav-number">1.4.3.</span> <span class="nav-text">Data Cleaning:处理noisy data方法:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Data-Integration-%E6%95%B0%E6%8D%AE%E6%95%B4%E5%90%88"><span class="nav-number">1.4.4.</span> <span class="nav-text">Data Integration(数据整合)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Data-Reduction"><span class="nav-number">1.4.5.</span> <span class="nav-text">Data Reduction</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99"><span class="nav-number">1.5.</span> <span class="nav-text">关联规则</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Apriori-%E7%AE%97%E6%B3%95-%E5%85%88%E9%AA%8C%E7%AE%97%E6%B3%95"><span class="nav-number">1.5.1.</span> <span class="nav-text">Apriori 算法(先验算法)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FP-Growth%E7%AE%97%E6%B3%95"><span class="nav-number">1.5.2.</span> <span class="nav-text">FP Growth算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Data-Classification"><span class="nav-number">1.6.</span> <span class="nav-text">Data Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91"><span class="nav-number">1.6.1.</span> <span class="nav-text">决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95ID3"><span class="nav-number">1.6.1.1.</span> <span class="nav-text">决策树算法ID3</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95C4-5"><span class="nav-number">1.6.1.2.</span> <span class="nav-text">决策树算法Ｃ4.5</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SVM"><span class="nav-number">1.6.2.</span> <span class="nav-text">SVM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KNN"><span class="nav-number">1.6.3.</span> <span class="nav-text">KNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bayesian-Networks-and-Classification"><span class="nav-number">1.6.4.</span> <span class="nav-text">Bayesian Networks and Classification</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.6.5.</span> <span class="nav-text">神经网络</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Genetic-Algorithms"><span class="nav-number">1.7.</span> <span class="nav-text">Genetic Algorithms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%B2%97%E7%B3%99%E9%9B%86"><span class="nav-number">1.8.</span> <span class="nav-text">粗糙集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Clustering-Algorithm-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95"><span class="nav-number">1.9.</span> <span class="nav-text">Clustering Algorithm (聚类算法)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#k-means"><span class="nav-number">1.9.1.</span> <span class="nav-text">k-means</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hierarchical-Clustering-%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB"><span class="nav-number">1.9.2.</span> <span class="nav-text">Hierarchical Clustering(层次聚类)</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">DoubleMagicXu</p>
  <div class="site-description" itemprop="description">晚上吃什么</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">16</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2017 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class=""></i>
  </span>
  <span class="author" itemprop="copyrightHolder">DoubleMagicXu</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
